{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import emoji\n",
    "import itertools \n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawToDf(file, key):\n",
    "    '''Converts raw .txt file into a Data Frame'''\n",
    "    \n",
    "    split_formats = {\n",
    "        '12hr' : '\\[\\d{1,2}/\\d{1,2}/\\d{2},\\s\\d{1,2}:\\d{2}:\\d{2}\\s[APap][mM]\\]\\s',\n",
    "        '24hr' : '\\[\\d{1,2}/\\d{1,2}/\\d{2},\\s\\d{1,2}:\\d{2}:\\d{2}\\]\\s',\n",
    "        'custom' : ''\n",
    "    }\n",
    "    datetime_formats = {\n",
    "        '12hr' : '[%d/%m/%y, %I:%M:%S %p] ',\n",
    "        '24hr' : '[%d/%m/%y, %H:%M:%S] ',\n",
    "        'custom': ''\n",
    "    }\n",
    "\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as raw_data:\n",
    "        raw_string = ' '.join(raw_data.read().split('\\n')) \n",
    "        user_msg = re.split(split_formats[key], raw_string) [1:] \n",
    "        date_time = re.findall(split_formats[key], raw_string)\n",
    "        \n",
    "        print(f'raw_string length: {len(raw_string)}')\n",
    "        print(f'user_msg length: {len(user_msg)}')\n",
    "        print(f'date_time length: {len(date_time)}')\n",
    "        \n",
    "        if len(user_msg) != len(date_time):\n",
    "            print('Mismatch between user_msg and date_time lengths')\n",
    "        \n",
    "        df = pd.DataFrame({'date_time': date_time, 'user_msg': user_msg})\n",
    "        \n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format=datetime_formats[key])\n",
    "    \n",
    "    usernames = []\n",
    "    msgs = []\n",
    "    for i in df['user_msg']:\n",
    "        a = re.split('([\\w\\W]+?):\\s', i) \n",
    "        if a[1:]: \n",
    "            usernames.append(a[1])\n",
    "            msgs.append(a[2])\n",
    "        else: \n",
    "            print(f'Unexpected format for i: {i}')\n",
    "            usernames.append(\"group_notification\")\n",
    "            msgs.append(a[0])\n",
    "\n",
    "    df['user'] = usernames\n",
    "    df['message'] = msgs\n",
    "\n",
    "    df.drop('user_msg', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_string length: 6368660\n",
      "user_msg length: 78040\n",
      "date_time length: 78040\n"
     ]
    }
   ],
   "source": [
    "df = rawToDf('cousins_chat.txt', '12hr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['date_time'].apply(lambda x: x.hour)\n",
    "df['day'] = df['date_time'].dt.strftime('%a')\n",
    "df['month'] = df['date_time'].dt.strftime('%b')\n",
    "df['year'] = df['date_time'].dt.year\n",
    "df['date'] = df['date_time'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_users = pd.read_csv('rename_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,rename_users,on='user',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('whatsapp_chat_formatted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()      # I will be using a copy of the original data frame everytime, to avoid loss of data!\n",
    "df1 = df1[df1['is_active'] == 1]\n",
    "df1['message_count'] = [1] * df1.shape[0]      # adding extra helper column --> message_count.\n",
    "df1.drop(columns='year', inplace=True)         # dropping unnecessary columns, using `inplace=True`, since this is copy of the DF and won't affect the original DataFrame.\n",
    "# df1 = df1.groupby('date').sum().reset_index()  \n",
    "df1 = df1.groupby('date')['message_count'].sum().reset_index() # grouping by date; since plot is of frequency of messages --> no. of messages / day.\n",
    "df1.to_pickle('df_daily_messages.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of messages per user\n",
    "df_no_of_messages = df.groupby('name').message.count().reset_index()\n",
    "\n",
    "df_no_of_messages.columns = ['name', 'number_of_msgs']\n",
    "\n",
    "df_no_of_messages.to_pickle('df_no_of_messages.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 most active days\n",
    "df2 = df.copy()      \n",
    "df2 = df2[df2['is_active'] == 1]\n",
    "df2['message_count'] = [1] * df2.shape[0] \n",
    "df2.drop(columns='year', inplace=True)    \n",
    "df2 = df2.groupby('date')['message_count'].sum().reset_index() \n",
    "\n",
    "df_top10days = df2.sort_values(by=\"message_count\", ascending=False).head(10)    # Sort values according to the number of messages per day.\n",
    "df_top10days.reset_index(inplace=True)           # reset index in order.\n",
    "df_top10days.drop(columns=\"index\", inplace=True) # dropping original indices.\n",
    "\n",
    "df_top10days.to_pickle('df_top10days.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most used emoji\n",
    "\n",
    "emoji_ctr = Counter()\n",
    "emojis_dict = emoji.get_emoji_unicode_dict('en')\n",
    "emojis_list = map(lambda x: ''.join(x.split()), emojis_dict.values())\n",
    "r = re.compile('|'.join(re.escape(p) for p in emojis_list))\n",
    "for idx, row in df.iterrows():\n",
    "    emojis_found = r.findall(row[\"message\"])\n",
    "    for emoji_found in emojis_found:\n",
    "        emoji_ctr[emoji_found] += 1\n",
    "\n",
    "df_top10emojis = pd.DataFrame()\n",
    "# top10emojis = pd.DataFrame(data, columns={\"emoji\", \"emoji_description\", \"emoji_count\"}) \n",
    "df_top10emojis['emoji'] = [''] * 10\n",
    "df_top10emojis['emoji_count'] = [0] * 10\n",
    "df_top10emojis['emoji_description'] = [''] * 10\n",
    "\n",
    "i = 0\n",
    "for item in emoji_ctr.most_common(10):\n",
    "    # will be using another helper column, since during visualization, the emojis won't be rendered.\n",
    "    description = emoji.demojize(item[0])[1:-1]    # using `[1:-1]` to remove the colons ':' at the end of the demojized strin\n",
    "    \n",
    "    # appending top 10 data of emojis.  # Loading into a DataFrame.\n",
    "    df_top10emojis.emoji[i] = item[0]\n",
    "    df_top10emojis.emoji_count[i] = int(item[1])\n",
    "    df_top10emojis.emoji_description[i] = description\n",
    "    i += 1\n",
    "\n",
    "df_top10emojis.to_pickle('df_top10emojis.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages by hour of the day\n",
    "df3 = df.copy()\n",
    "df3['message_count'] = [1] * df.shape[0]    # helper column to keep a count.\n",
    "\n",
    "df3['hour'] = df3['date_time'].apply(lambda x: x.hour)\n",
    "\n",
    "df_by_hour = df3.groupby('hour')['message_count'].sum().reset_index().sort_values(by = 'hour')\n",
    "\n",
    "df_by_hour.to_pickle('df_by_hour.pkl')\n",
    "\n",
    "# Messages by day\n",
    "df_by_day = df3.groupby('day')['message_count'].sum().reset_index().sort_values(by = 'day')\n",
    "df_by_day.to_pickle('df_by_day.pkl')\n",
    "\n",
    "# Messages by month\n",
    "df_by_month = df3.groupby('month')['message_count'].sum().reset_index().sort_values(by = 'month')\n",
    "df_by_month.to_pickle('df_by_month.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment_words = ' '\n",
    "\n",
    "# # stopwords --> Words to be avoided while forming the WordCloud,\n",
    "# # removed group_notifications like 'joined', 'deleted';\n",
    "# # removed really common words like \"yeah\" and \"okay\".\n",
    "# stopwords = STOPWORDS.update(['group', 'link', 'invite', 'joined', 'message', 'deleted', 'yeah', 'hai', 'yes', 'okay', 'ok', 'will', 'use', 'using', 'one', 'know', 'guy', 'group', 'media', 'omitted','image'])\n",
    "\n",
    "\n",
    "# # iterate through the DataFrame.\n",
    "# for val in df3.message.values:\n",
    "    \n",
    "#     # typecaste each val to string.\n",
    "#     val = str(val) \n",
    "    \n",
    "#     # split the value.\n",
    "#     tokens = val.split() \n",
    "    \n",
    "#     # Converts each token into lowercase.\n",
    "#     for i in range(len(tokens)): \n",
    "#         tokens[i] = tokens[i].lower() \n",
    "          \n",
    "#     for words in tokens: \n",
    "#         comment_words = comment_words + words + ' '\n",
    "  \n",
    "  \n",
    "# wordcloud = WordCloud(width = 600, height = 600, \n",
    "#                 background_color ='white', \n",
    "#                 stopwords = stopwords, \n",
    "#                 min_font_size = 8).generate(comment_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['group_name'] = df['message'].str.extract('changed the subject to “([^”]*).*')\n",
    "df_group_name = df[df['message'].str.contains('subject to', na=False)]\n",
    "df_group_name['name'] = df_group_name['name'].fillna('Battu')\n",
    "\n",
    "df_group_name.to_pickle('df_group_name.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Created: 2023-06-02 17:53:14.812162\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "file_path = \"cousins_chat.txt\"  # replace with your file path\n",
    "\n",
    "# Get file creation time\n",
    "creation_time = os.path.getctime(file_path)\n",
    "\n",
    "# Convert to readable timestamp\n",
    "creation_date = datetime.datetime.fromtimestamp(creation_time)\n",
    "\n",
    "print(\"File Created: \" + str(creation_date))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
